# Urdu-Tokenization-using-Google-Sentencepiece
We used different NLP and machine learning tools to train model and then to tokenize Urdu text. We have also used Google Sentence Piece, which is an unsupervised text tokenizer which implements sub word units and does not depend on language-specific pre/post-processing. 
